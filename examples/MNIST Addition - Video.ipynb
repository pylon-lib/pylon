{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task considered in this notebook is very reminiscent of the classical learning task on the MNIST data. However, instead of providing labels for single digits, we train on pairs of images labeled with the sum of the individual digits. It was first introduced in Manhaeve 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanliying/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fabfcfe9030>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining our model, taken from the Pytorch MNIST tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self, N=10):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1,  6, 5),\n",
    "            nn.MaxPool2d(2, 2), # 6 24 24 -> 6 12 12\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(6, 16, 5), # 6 12 12 -> 16 8 8\n",
    "            nn.MaxPool2d(2, 2), # 16 8 8 -> 16 4 4\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.classifier =  nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, N)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the usual MNIST image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:01, 6405934.93it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 7006866.09it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1649664it [00:00, 18619900.63it/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 2585772.00it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_train_data = torchvision.datasets.MNIST(root='./MNIST', train=True, download=True,transform=transform)\n",
    "mnist_test_data = torchvision.datasets.MNIST(root='./MNIST', train=False, download=True,transform=transform)\n",
    "\n",
    "test_kwargs = {'batch_size': 256}\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test_data, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the MNIST addition dataset, generated by pairing random MNIST digits and labeling them with their summation i.e. each datum is of the form (idx1, idx2, summation) where idx1 corresponds to the index of the first image, idx2 corresponds to the index of the second image, and summation corresponds to the sum of their groundtruth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- train_data ----------\n",
    "with open('train_data.txt') as f:\n",
    "    train_data = f.readlines()\n",
    "    \n",
    "# Strip new lines\n",
    "train_data = [d.strip() for d in train_data]\n",
    "\n",
    "# Convert strings (e.g. \"(datum_i, datum_j, sum)\") to tuples of ints\n",
    "train_data = [tuple(int(e) for e in d.strip(\"()\").split(\",\")) for d in train_data]\n",
    "\n",
    "# ---------- test data ----------\n",
    "with open('test_data.txt') as f:\n",
    "    test_data = f.readlines()\n",
    "    \n",
    "# Strip new lines\n",
    "test_data = [d.strip() for d in test_data]\n",
    "\n",
    "# Convert strings (e.g. \"(datum_i, datum_j, sum)\") to tuples of ints\n",
    "test_data = [tuple(int(e) for e in d.strip(\"()\").split(\",\")) for d in test_data]\n",
    "\n",
    "# Tensorize\n",
    "train_data = torch.tensor(train_data)[:9000]\n",
    "test_data = torch.tensor(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our model as well as our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we train on pairs of images and their summation, we test on the classic setting i.e. predicting the label of a single digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lieu of the traditional cross entropy loss, we require that the sum of predicted labels match the groundtruth by enforcing it as a constraint at training time. This requires that we import the *constraint* module. Line 12 declares *enforce_sum_constraint* as a constraint to be enforced at training time. We note that our constraint function, *enforce_sum* is a vanilla python function, and does not make use of any foreign syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Set up the constraints ----------\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pylon.constraint import constraint\n",
    "\n",
    "def enforce_sum(img1, img2, **kwargs):\n",
    "    return img1 + img2 == kwargs['summation']\n",
    "\n",
    "\n",
    "enfore_sum_constraint = constraint(enforce_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we proceed to our normal training loop, where we minimize our constraint loss during training, as can be seen on line 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9000 [00:00<?, ?it/s]/Users/hanliying/Documents/UCLA/Research/pylon/pylon/examples/../pylon/brute_force_solver.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  else torch.tensor(data=self.cond(*sample, **kwargs), dtype=torch.bool) for sample in samples ])\n",
      " 11%|█         | 1005/9000 [00:54<25:23,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 6328/10000 (63%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2010/9000 [01:22<11:46,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 9310/10000 (93%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3009/9000 [02:01<13:27,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 9530/10000 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4009/9000 [02:27<11:41,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 9650/10000 (96%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5009/9000 [02:57<11:37,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 9472/10000 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6007/9000 [03:30<08:38,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 9681/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7008/9000 [04:00<04:04,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 9659/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8006/9000 [04:26<01:35, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 9593/10000 (96%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [04:52<00:00, 30.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 9669/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "      \n",
    "    # train\n",
    "    for i, batch in enumerate(tqdm(train_data)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        idx1, idx2, summation = batch\n",
    "        X1 = mnist_train_data[idx1][0].unsqueeze(0)\n",
    "        X2 = mnist_train_data[idx2][0].unsqueeze(0)\n",
    "        \n",
    "        output1 = model(X1)\n",
    "        output2 = model(X2)\n",
    "        \n",
    "        pred1 = output1.argmax(dim=1, keepdim=False)\n",
    "        pred2 = output2.argmax(dim=1, keepdim=False)\n",
    "        \n",
    "        closs = enfore_sum_constraint(output1, output2, summation=summation)\n",
    "\n",
    "\n",
    "        closs.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            test()\n",
    "        \n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a719078278c4492d805b55cadf911a94c7633b48007fee7a7c47218b923b9ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
