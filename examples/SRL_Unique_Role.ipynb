{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import *\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch import nn\n",
    "from pytorch_constraints.constraint import constraint\n",
    "from pytorch_constraints.sampling_solver import WeightedSamplingSolver\n",
    "#from pytorch_constraints.circuit_solver import SemanticLossCircuitSolver\n",
    "from pytorch_constraints.shaped_lazy_solver import ProductTNormSolver\n",
    "from pytorch_constraints.shaped_lazy_solver import GodelTNormSolver\n",
    "from pytorch_constraints.shaped_lazy_solver import LukasiewiczTNormSolver\n",
    "\n",
    "#B_A0, B_A1, B_A2, B_A3, B_A4 = None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ex(tokenizer, labels, orig_toks, v_labels, role_labels, max_seq_l=200, max_v_num=8, max_num_subtok=8):\n",
    "    def pad(ls, length, symbol):\n",
    "        if len(ls) >= length:\n",
    "            return ls[:length]\n",
    "        return ls + [symbol] * (length -len(ls))\n",
    "\n",
    "    bos_tok, eos_tok = tokenizer.cls_token, tokenizer.sep_token\n",
    "    assert(bos_tok is not None and eos_tok is not None)\n",
    "    assert(len(v_labels) == len(role_labels))\n",
    "\n",
    "    idx = sorted([(v, i) for i, v in enumerate(v_labels)])\n",
    "    idx = [p[1] for p in idx]\n",
    "    v_labels = [v_labels[i] for i in idx]\n",
    "    role_labels = [role_labels[i] for i in idx]\n",
    "    role_labels = [[labels.index(l) for l in r] for r in role_labels]\n",
    "\n",
    "    # trimming\n",
    "    orig_toks = orig_toks[:max_seq_l - 2]\n",
    "    v_labels_new, role_labels_new = [],[]\n",
    "    for v_idx, roles in zip(v_labels, role_labels):\n",
    "        if v_idx >= max_seq_l - 2 or len(v_labels_new) >= max_v_num:\n",
    "            break\n",
    "        v_labels_new.append(v_idx)\n",
    "        role_labels_new.append(roles)\n",
    "    v_labels, role_labels = v_labels_new, role_labels_new\n",
    "\n",
    "    # subtoks\n",
    "    sent_subtoks = [tokenizer.tokenize(t) for t in orig_toks]\n",
    "    tok_l = [len(subtoks) for subtoks in sent_subtoks]\n",
    "    toks = [p for subtoks in sent_subtoks for p in subtoks] # flatterning\n",
    "\n",
    "    # pad for CLS and SEP\n",
    "    toks = [bos_tok] + toks + [eos_tok]\n",
    "    tok_l = [1] + tok_l + [1]\n",
    "    orig_toks = [bos_tok] + orig_toks + [eos_tok]\n",
    "    v_labels = [l+1 for l in v_labels]  # incr v pos for CLS\n",
    "\n",
    "    # padding\n",
    "    tok_idx = tokenizer.convert_tokens_to_ids(toks)\n",
    "    tok_idx = pad(tok_idx, max_seq_l, tokenizer.pad_token_id)\n",
    "    v_labels = pad(v_labels, max_v_num, -1)\n",
    "    role_labels = [pad(r, max_seq_l, 0) for r in role_labels]\n",
    "    role_labels = pad(role_labels, max_v_num, [0]*max_seq_l)\n",
    "\n",
    "    #\n",
    "    acc = 0\n",
    "    sub2tok_idx = []\n",
    "    for l in tok_l:\n",
    "        sub2tok_idx.append(pad([p for p in range(acc, acc+l)], max_num_subtok, -1))\n",
    "        assert(len(sub2tok_idx[-1]) <= max_num_subtok)\n",
    "        acc += l\n",
    "    sub2tok_idx = pad(sub2tok_idx, max_seq_l, [-1 for _ in range(max_num_subtok)])\n",
    "\n",
    "    return tok_idx, sub2tok_idx, v_labels, role_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tokenizer, path):\n",
    "    def group_ex(data):\n",
    "        rs = {}\n",
    "        for orig_toks, v_idx, role_labels in data:\n",
    "            key = ' '.join(orig_toks)\n",
    "            if key not in rs:\n",
    "                rs[key] = {'v_labels': [], 'role_labels': []}\n",
    "            rs[key]['v_labels'].append(v_idx)\n",
    "            rs[key]['role_labels'].append(role_labels)\n",
    "        return rs\n",
    "\n",
    "    def batch_process(grouped, labels):\n",
    "        rs = []\n",
    "        for key, pack in grouped.items():\n",
    "            orig_toks = key.split(' ')\n",
    "            (tok_idx, sub2tok_idx, v_labels, role_labels) = process_ex(tokenizer, labels, orig_toks, pack['v_labels'], pack['role_labels'])\n",
    "            rs.append((tok_idx, sub2tok_idx, v_labels, role_labels))\n",
    "        return (torch.tensor([p[0] for p in rs], dtype=torch.long), \n",
    "                torch.tensor([p[1] for p in rs], dtype=torch.long), \n",
    "                torch.tensor([p[2] for p in rs], dtype=torch.long), \n",
    "                torch.tensor([p[3] for p in rs], dtype=torch.long))\n",
    "\n",
    "\n",
    "    labels = []\n",
    "    with open(path + '/labels.txt', 'r') as f:\n",
    "        for l in f:\n",
    "            if l.strip() == '':\n",
    "                continue\n",
    "            labels.append(l.strip())\n",
    "\n",
    "    files = ['srl.train.txt', 'srl.test.txt']\n",
    "    all_data = []\n",
    "    for file in files:\n",
    "        all_data.append([])\n",
    "        file = path + '/' + file\n",
    "        with open(file, 'r') as f:\n",
    "            print('loading from', file)\n",
    "            for line in f:\n",
    "                if line.strip() == '':\n",
    "                    continue\n",
    "                parts = line.split('|||')\n",
    "                assert(len(parts) == 2)\n",
    "                v_idx = int(parts[0].strip().split()[0])\n",
    "                orig_toks = parts[0].strip().split()[1:]\n",
    "                roles = parts[1].strip().split()\n",
    "                assert(len(orig_toks) == len(roles))\n",
    "                all_data[-1].append((orig_toks, v_idx, roles))\n",
    "\n",
    "    # group examples by orig_toks\n",
    "    grouped = group_ex(all_data[0])\n",
    "    tok_idx, sub2tok_idx, v_labels, role_labels = batch_process(grouped, labels)\n",
    "    train = TensorDataset(tok_idx, sub2tok_idx, v_labels, role_labels)\n",
    "    \n",
    "    grouped = group_ex(all_data[1])\n",
    "    tok_idx, sub2tok_idx, v_labels, role_labels = batch_process(grouped, labels)\n",
    "    test = TensorDataset(tok_idx, sub2tok_idx, v_labels, role_labels)\n",
    "    return train, test, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, batch_size=8, device=torch.device('cpu')):\n",
    "    test_data_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)\n",
    "    ex_cnt = 0\n",
    "    predicate_cnt = 0\n",
    "    iou_accumulator = 0.0\n",
    "    global_sat = 0.0\n",
    "    for _, batch in enumerate(test_data_loader):\n",
    "        with torch.no_grad():\n",
    "            tok_idx, sub2tok_idx, v_labels, role_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
    "            _, logits = model(tok_idx, sub2tok_idx, v_labels, role_labels)\n",
    "\n",
    "            batch_l = tok_idx.shape[0]\n",
    "            orig_l = (sub2tok_idx[:, :, 0] != -1).sum(-1).long()\n",
    "            v_l = (v_labels != -1).sum(-1)\n",
    "            for i in range(batch_l):\n",
    "                v_i = v_labels[i, :v_l[i]]\n",
    "                y_pred = logits[i, v_i, :orig_l[i]].max(-1)[1]    # use label index here\n",
    "                satisfied = unique_role_check(y_pred)\n",
    "                global_sat += float(satisfied.all(-1).sum())    # we want for each predicate, all tokens satisfy the constraint\n",
    "                predicate_cnt += int(v_l[i])\n",
    "\n",
    "            # quick stats\n",
    "            orig_l = (sub2tok_idx[:, :, 0] != -1).sum(-1).long()\n",
    "            v_l = (v_labels != -1).sum(-1)\n",
    "            for i in range(batch_l):\n",
    "                v_i = v_labels[i, :v_l[i]]\n",
    "                p = logits[i, v_i, :orig_l[i]].argmax(-1)\n",
    "                g = role_labels[i, :v_l[i], :orig_l[i]]\n",
    "                if p.sum() > 0 or g.sum() > 0:\n",
    "                    # get intersection over union without counting mutual O's\n",
    "                    intersection = (p.masked_fill(p==0, -1) == g.masked_fill(g==0, -2)).sum()\n",
    "                    union = ((p != 0) + (g != 0) > 0).sum()\n",
    "                    iou_accumulator += float(intersection/union) if union > 0 else 0\n",
    "                else:\n",
    "                    iou_accumulator += 0\n",
    "            ex_cnt += batch_l\n",
    "\n",
    "    print('test set iou', iou_accumulator/ex_cnt)\n",
    "    print('Global percent of predicates that violate the unique core role constraint', 1-global_sat/predicate_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRLModel(torch.nn.Module):\n",
    "    def __init__(self, t_type, labels):\n",
    "        super(SRLModel, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(t_type)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(t_type)\n",
    "\n",
    "        self.labels = labels\n",
    "        self.num_label = len(labels)\n",
    "        self.hidden_size = self.transformer.config.hidden_size\n",
    "        self.g_va = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size*2, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU())\n",
    "        self.f_v = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.f_a = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.label_layer = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.hidden_size, self.num_label))\n",
    "\n",
    "    # use the idx (batch_l, seq_l, rs_l) (2nd dim) to select the middle dim of the content (batch_l, seq_l, d)\n",
    "    #   the result has shape (batch_l, seq_l, rs_l, d)\n",
    "    def batch_index2_select(self, content, idx, nul_idx):\n",
    "        idx = idx.long()\n",
    "        rs_l = idx.shape[-1]\n",
    "        batch_l, seq_l, d = content.shape\n",
    "        content = content.contiguous().view(-1, d)\n",
    "        shift = torch.arange(0, batch_l).to(idx.device).long().view(batch_l, 1, 1)\n",
    "        shift = shift * seq_l\n",
    "        shifted = idx + shift\n",
    "        rs = content[shifted].view(batch_l, seq_l, rs_l, d)\n",
    "        mask = (idx != nul_idx).unsqueeze(-1)\n",
    "        return rs * mask.to(rs)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, sub2tok_idx, v_labels, role_labels):\n",
    "        enc = self.transformer(input_ids=input_ids, return_dict=True).last_hidden_state\n",
    "        enc = self.batch_index2_select(enc, sub2tok_idx, nul_idx=-1).sum(2) # summing subtoks -> (batch_l, seq_l, hidden_size)\n",
    "        (batch_l, seq_l, hidden_size) = enc.shape\n",
    "\n",
    "        v_enc = self.f_v(enc.view(-1, self.hidden_size)).view(batch_l, seq_l, 1, self.hidden_size)\n",
    "        a_enc = self.f_a(enc.view(-1, self.hidden_size)).view(batch_l, 1, seq_l, self.hidden_size)\n",
    "\n",
    "        va_enc = torch.cat([\n",
    "            v_enc.expand(batch_l, seq_l, seq_l, self.hidden_size),\n",
    "            a_enc.expand(batch_l, seq_l, seq_l, self.hidden_size)], dim=-1)\n",
    "        va_enc = self.g_va(va_enc.view(-1, self.hidden_size*2))\n",
    "        va_enc = va_enc.view(batch_l, seq_l, seq_l, self.hidden_size)\n",
    "\n",
    "        logits = self.label_layer(va_enc.view(-1, hidden_size)).view(batch_l, seq_l, seq_l, self.num_label)\n",
    "\n",
    "        # loss using gold predicates\n",
    "        loss = torch.zeros(1, device=device)\n",
    "        orig_l = (sub2tok_idx[:, :, 0] != -1).sum(-1).long()\n",
    "        v_l = (v_labels != -1).sum(-1)\n",
    "        for i in range(batch_l):\n",
    "            v_i = v_labels[i, :v_l[i]]\n",
    "            p = logits[i, v_i, :orig_l[i]].contiguous()\n",
    "            g = role_labels[i, :v_l[i], :orig_l[i]].contiguous()\n",
    "            loss += torch.nn.CrossEntropyLoss(reduction='mean')(p.view(-1, self.num_label), g.view(-1))\n",
    "        loss = loss / batch_l\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_role(y):\n",
    "    from pytorch_constraints import lazy_torch as torch\n",
    "    shape = y.size()\n",
    "    y_ext = y + 1e-6\n",
    "    y_ext = y_ext.unsqueeze(1).tile(1, shape[1], 1, 1).log()\n",
    "    y_ext = y_ext + torch.eye(shape[1]).unsqueeze(0).unsqueeze(3).tile(shape[0], 1, 1, shape[2]) * -1e6\n",
    "    y_ext = y_ext.exp()\n",
    "\n",
    "    b_a0 = y[:, :, B_A0] <= y_ext[:, :, :, B_A0].logical_not().all(2)\n",
    "    b_a1 = y[:, :, B_A1] <= y_ext[:, :, :, B_A1].logical_not().all(2)\n",
    "    b_a2 = y[:, :, B_A2] <= y_ext[:, :, :, B_A2].logical_not().all(2)\n",
    "    b_a3 = (y[:, :, B_A3]) <= y_ext[:, :, :, B_A3].logical_not().all(2)\n",
    "    b_a4 = (y[:, :, B_A4]) <= y_ext[:, :, :, B_A4].logical_not().all(2)\n",
    "    return b_a0.logical_and(b_a1).logical_and(b_a2).logical_and(b_a3).logical_and(b_a4)\n",
    "\n",
    "# TODO, use unique_role function instead\n",
    "def unique_role_check(y_pred):\n",
    "    batch_l, seq_l = y_pred.shape\n",
    "    y_ext = y_pred.view(batch_l, 1, seq_l).expand(batch_l, seq_l, seq_l)\n",
    "    y_ext = y_ext.clone()    # make a copy since we gonna mask out diagonal in place\n",
    "    y_ext.diagonal(0,1,2).zero_()    # mask out the diagonal to be 'O'\n",
    "\n",
    "    satisfied = torch.ones(y_pred.shape, device=y_pred.device).bool()\n",
    "    for label in [B_A0, B_A1, B_A2, B_A3, B_A4]:\n",
    "        lhs = (y_pred == label)\n",
    "        rhs = (y_ext == label).logical_not().all(2)\n",
    "        sat = lhs.logical_not().logical_or(rhs)\n",
    "        satisfied = satisfied.logical_and(sat)\n",
    "    return satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, solver, train_data, \n",
    "            lr=5e-5, batch_size=8, seed=1, grad_clip=1.0, lambda_constr=0.1, epoch=1,\n",
    "            use_constr=False, device=torch.device('cpu')):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    train_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
    "\n",
    "    # mixing two datasets\n",
    "    data_loaders = [train_loader]\n",
    "    expanded_data_loader = [train_loader] * len(train_loader)\n",
    "    random.shuffle(expanded_data_loader)\n",
    "\n",
    "    # create optimizer\n",
    "    weight_decay = 0\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    named_params = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "    total_updates = epoch * len(expanded_data_loader)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_updates)\n",
    "\n",
    "    update_cnt = 0\n",
    "    loss_accumulator = 0.0\n",
    "    model = model.to(device)\n",
    "    model.zero_grad()\n",
    "    for epoch_id in range(epoch):\n",
    "        iters = [loader.__iter__() for loader in data_loaders]\n",
    "        for loader in expanded_data_loader:\n",
    "            batch = next(iters[data_loaders.index(loader)])\n",
    "\n",
    "            tok_idx, sub2tok_idx, v_labels, role_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
    "\n",
    "            loss, logits = model(tok_idx, sub2tok_idx, v_labels, role_labels)\n",
    "\n",
    "            if use_constr:\n",
    "                constrain_func = constraint(unique_role, solver)\n",
    "                batch_l = logits.shape[0]\n",
    "                orig_l = (sub2tok_idx[:, :, 0] != -1).sum(-1).long()\n",
    "                v_l = (v_labels != -1).sum(-1)\n",
    "                for i in range(batch_l):\n",
    "                    v_i = v_labels[i, :v_l[i]]\n",
    "                    p = logits[i, v_i, :orig_l[i]]\n",
    "                    c_loss = constrain_func(p)\n",
    "                    loss = loss + c_loss * lambda_constr / v_l[i]\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss_accumulator += (loss.item())\n",
    "            update_cnt += 1\n",
    "\n",
    "            if update_cnt % 100 == 0:\n",
    "                print('trained {0} steps, avg loss {1:4f}'.format(update_cnt, float(loss_accumulator/update_cnt)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')\n",
    "device = torch.device(\"cuda\", 1)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# data\n",
    "print('processing data...')\n",
    "train_data, test_data, labels = process_data(tokenizer, './examples/srl/')\n",
    "B_A0 = labels.index('B-ARG0')\n",
    "B_A1 = labels.index('B-ARG1')\n",
    "B_A2 = labels.index('B-ARG2')\n",
    "B_A3 = labels.index('B-ARG3')\n",
    "B_A4 = labels.index('B-ARG4')\n",
    "\n",
    "#\n",
    "print('initializing models and solvers...')\n",
    "solver = ProductTNormSolver()\n",
    "model = SRLModel('distilbert-base-uncased', labels)\n",
    "\n",
    "# train\n",
    "print('training on gold data...')\n",
    "model = train(model, solver, train_data, lr=5e-5, epoch=2, use_constr=False, batch_size=6, device=device)\n",
    "\n",
    "print('evaluating on test set...')\n",
    "evaluate(model, test_data, device=device)\n",
    "\n",
    "solver = ProductTNormSolver()\n",
    "model = SRLModel('distilbert-base-uncased', labels)\n",
    "\n",
    "# train\n",
    "print('training on gold data with constraint...')\n",
    "model = train(model, solver, train_data, lr=5e-5, epoch=2, use_constr=True, batch_size=6, lambda_constr=0.01, device=device)\n",
    "\n",
    "print('evaluating on test set...')\n",
    "evaluate(model, test_data, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
